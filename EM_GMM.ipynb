{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_GMM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mz-zarei/MNIST_Classification/blob/main/EM_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T41g2zO81gqI",
        "outputId": "74c56980-e25e-4653-a587-3f04676f6a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "\"\"\"\n",
        "Author: Mohammad Zarei\n",
        "Expectation-Maximaization for Gaussian Mixture Models\n",
        "Last update: 2016-11-30\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "from keras.datasets import mnist\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.special import logsumexp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pozspw-MmRmb"
      },
      "source": [
        "eps = 0.1  #to add the numbers that are being devided\n",
        "\n",
        "\n",
        "def multivariate_gaussian(X, pi, variances, means):\n",
        "  \"\"\"\n",
        "  X: examples with shape (n, d)\n",
        "  pi:  priors with shape (K, )\n",
        "  variances and means shape: (K, d)\n",
        "  log_r_matrix: log of r matrix with shape (K, n)\n",
        "  \"\"\"\n",
        "  reversed_var = 1 / variances\n",
        "  log_r_matrix = (X ** 2) @ reversed_var.T\n",
        "  log_r_matrix -= 2 * X @ (means * reversed_var).T\n",
        "  log_r_matrix[:] += np.sum((means ** 2) * reversed_var, axis=1)\n",
        "  log_r_matrix *= -0.5\n",
        "  log_r_matrix[:] += np.log(pi) - 0.5 * np.sum(np.log(variances), axis=1)\n",
        "  log_r_matrix = log_r_matrix.T\n",
        "\n",
        "  sum_log_r = logsumexp(log_r_matrix, axis=0)\n",
        "  return sum_log_r, log_r_matrix\n",
        "\n",
        "\n",
        "\n",
        "def em_gmm(X, gm_num):\n",
        "  max_iter = 500\n",
        "  tol = 1e-5\n",
        "\n",
        "  # X dimensions\n",
        "  n, d = X.shape\n",
        "  # log-likelihood values of each iteration\n",
        "  log_likelihood_loss = {}\n",
        "\n",
        "  # initialization\n",
        "  log_r_matrix = np.eye(gm_num)\n",
        "  log_r_matrix = log_r_matrix[np.random.choice(gm_num, size=n)].T\n",
        "  means = variances = pi = np.array([]) \n",
        "  iter_num = 0\n",
        "\n",
        "    \n",
        "  for it in range(max_iter):\n",
        "    # M-Step\n",
        "    r_k = np.sum(log_r_matrix, axis=1) + (10 * np.finfo(float).eps)  # shape: (K,), sum of elements in log_r_matrix rows\n",
        "    pi = r_k / n\n",
        "    means = log_r_matrix @ X / r_k[:, np.newaxis]  # (K, d)\n",
        "    variances = log_r_matrix @ (X ** 2) / r_k[:, np.newaxis]\n",
        "    variances -= means ** 2\n",
        "    variances += eps         #avoiding zero elements for future by-zero division\n",
        "\n",
        "    # E-Step\n",
        "    sum_log_r, log_r_matrix = multivariate_gaussian(X, pi, variances, means)\n",
        "    log_r_matrix = np.exp(log_r_matrix - sum_log_r)\n",
        "\n",
        "    # compute loss\n",
        "    log_likelihood_loss[it] = -np.sum(sum_log_r)\n",
        "    loss_difference = 0\n",
        "    if it > 1:\n",
        "      loss_difference = np.abs(log_likelihood_loss[it] - log_likelihood_loss[it - 1]) / (np.abs(log_likelihood_loss[it]) + eps)\n",
        "      if loss_difference <= tol:\n",
        "        iter_num = it\n",
        "        break     \n",
        "\n",
        "  print(\"EM for GMM converged after \", iter_num + 1, \"iteration, with loss: \", log_likelihood_loss[iter_num])\n",
        "  GMM_Params = {'log_r_matrix': log_r_matrix, 'means': means, 'variances': variances, 'pi': pi}\n",
        "  return GMM_Params, log_likelihood_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDHrnQy1nJ6Z",
        "outputId": "a06978cb-263b-4022-8b18-7e01911bdf81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "##### MNIST prediction using EM-GMM #####\n",
        "\n",
        "effective_features = 50   # number top effective features to be selected by PCA\n",
        "gm_num = 5                #number of Gaussian Models in each GMM\n",
        "\n",
        "\n",
        "# Loading and normalizing MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print('Data loaded!')\n",
        "\n",
        "#flattening feature dataset\n",
        "X_train = X_train.astype(float).reshape(-1, 28 * 28) / 255\n",
        "X_test = X_test.astype(float).reshape(-1, 28 * 28) / 255\n",
        "\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "\n",
        "num_classes = 10\n",
        "n = X_train.shape[0]\n",
        "\n",
        "# Implementing PCA\n",
        "\n",
        "pca = PCA(n_components = effective_features)\n",
        "pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "print(\"New dimension after PCA: \", X_train.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "GMM_Params = {}\n",
        "log_likelihood_loss = {}\n",
        "prior = np.zeros(num_classes)\n",
        "\n",
        "for digit in range(num_classes):\n",
        "  print('... GMM is fitting to digit  ', digit)\n",
        "  X = X_train[y_train == digit]\n",
        "  GMM_Params[digit], log_likelihood_loss[digit] = em_gmm(X, gm_num)\n",
        "  print('GMM parameters computed for digit = ', digit)\n",
        "  prior[digit] = (X.shape[0] / n)\n",
        "\n",
        "print('....GMM parameteres for each digit were computed!')\n",
        "\n",
        "\n",
        "print('predicting test labels ...')\n",
        "num_test = X_test.shape[0]\n",
        "class_probab_list = np.zeros((num_classes, num_test))\n",
        "\n",
        "for digit in range(num_classes):\n",
        "  sum_log_r, r = multivariate_gaussian(X_test, GMM_Params[digit]['pi'], GMM_Params[digit]['variances'], GMM_Params[digit]['means'])\n",
        "  class_probab_list[digit] = logsumexp(r, axis=0)*prior[digit]\n",
        "\n",
        "predictions = np.argmax(class_probab_list, axis=0)\n",
        "\n",
        "correct_predictions = np.sum(predictions == y_test)\n",
        "accuracy = correct_predictions / num_test\n",
        "\n",
        "\n",
        "print(\"accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data loaded!\n",
            "X_train shape:  (60000, 784)\n",
            "X_test shape:  (10000, 784)\n",
            "New dimension after PCA:  50\n",
            "... GMM is fitting to digit   0\n",
            "EM for GMM converged after  73 iteration, with loss:  27060.333533019762\n",
            "GMM parameters computed for digit =  0\n",
            "... GMM is fitting to digit   1\n",
            "EM for GMM converged after  32 iteration, with loss:  -150641.32289929048\n",
            "GMM parameters computed for digit =  1\n",
            "... GMM is fitting to digit   2\n",
            "EM for GMM converged after  37 iteration, with loss:  52918.7493264753\n",
            "GMM parameters computed for digit =  2\n",
            "... GMM is fitting to digit   3\n",
            "EM for GMM converged after  34 iteration, with loss:  37675.2406679393\n",
            "GMM parameters computed for digit =  3\n",
            "... GMM is fitting to digit   4\n",
            "EM for GMM converged after  97 iteration, with loss:  20874.178138678366\n",
            "GMM parameters computed for digit =  4\n",
            "... GMM is fitting to digit   5\n",
            "EM for GMM converged after  47 iteration, with loss:  32984.08383414702\n",
            "GMM parameters computed for digit =  5\n",
            "... GMM is fitting to digit   6\n",
            "EM for GMM converged after  70 iteration, with loss:  12626.070621397532\n",
            "GMM parameters computed for digit =  6\n",
            "... GMM is fitting to digit   7\n",
            "EM for GMM converged after  157 iteration, with loss:  -7722.167622220802\n",
            "GMM parameters computed for digit =  7\n",
            "... GMM is fitting to digit   8\n",
            "EM for GMM converged after  44 iteration, with loss:  41426.700069499486\n",
            "GMM parameters computed for digit =  8\n",
            "... GMM is fitting to digit   9\n",
            "EM for GMM converged after  303 iteration, with loss:  412.2033341420299\n",
            "GMM parameters computed for digit =  9\n",
            "....GMM parameteres for each digit were computed!\n",
            "predicting test labels ...\n",
            "accuracy:  0.9202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTkOW2IpoEQ5",
        "outputId": "c06cf622-6636-4a29-d6b0-9a52f8aff84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "#### Accuracy with sklearn\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "gmm_list = []\n",
        "prior_list = []\n",
        "num_test = X_test.shape[0]\n",
        "class_probab_list = np.zeros((num_classes, num_test))\n",
        "\n",
        "for digit in range(num_classes):\n",
        "    print(digit)\n",
        "    X = X_train[y_train == digit]\n",
        "    gm = GMM(n_components=5, covariance_type = 'diag', max_iter=500, tol=1e-5).fit(X)\n",
        "    gmm_list.append(gm)\n",
        "    prior_list.append(X.shape[0]/X_train.shape[0])\n",
        "    sum_log_r, r = multivariate_gaussian(X_test, gm.weights_, gm.covariances_, gm.means_)\n",
        "    class_probab_list[digit] = logsumexp(r, axis=0)* X.shape[0]/X_train.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "predictions = np.argmax(class_probab_list, axis=0)\n",
        "\n",
        "correct_predictions = np.sum(predictions == y_test)\n",
        "accuracy = correct_predictions / num_test\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "0.916\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}